# ImageProcess/DetectionTest.py - å¹³è¡¡ç‰ˆæœ¬

import os
import sys
import cv2
import numpy as np
import time
import random
from collections import deque
from ImageProcess.ScreenCapture import capture_bluestacks

class BalancedDetector:
    """å¹³è¡¡ç‰ˆæª¢æ¸¬å™¨ - é¿å…éåº¦åš´æ ¼æˆ–éåº¦å¯¬é¬†"""
    
    def __init__(self, template_dir='templates\\monsters'):
        self.template_dir = template_dir
        self.templates = []
        
        # ä¿å­˜è¨­å®š
        self.save_detection_images = True
        self.save_directory = "detection_results"
        self.create_save_directory()
        
        # âœ… åŸºæ–¼æœç´¢çµæœ[1]çš„å¹³è¡¡åƒæ•¸
        self.confidence_threshold = 0.04        # å¾0.12é™åˆ°0.06
        self.spatial_distance_threshold = 80   # é©ä¸­çš„èšé¡è·é›¢
        
        # âœ… é©åº¦çš„æ¨¡æ¿éæ¿¾
        self.enable_roi_templates = True        # ä¿æŒåœç”¨ROIæ¨¡æ¿
        self.enable_detection_history = True
        self.detection_history = deque(maxlen=3)
        
        # èƒŒæ™¯è™•ç†è¨­å®š
        self.enable_background_removal = True
        self.bg_removal_method = 'gentle_enhancement'
        
        # åˆå§‹åŒ–æª¢æ¸¬å™¨
        self.use_sift = self._init_detector()
        self._setup_matcher()
        self._load_balanced_templates()
    
    def create_save_directory(self):
        """å‰µå»ºä¿å­˜ç›®éŒ„"""
        try:
            if not os.path.exists(self.save_directory):
                os.makedirs(self.save_directory)
                print(f"âœ… å‰µå»ºä¿å­˜ç›®éŒ„: {self.save_directory}")
        except Exception as e:
            print(f"âŒ å‰µå»ºä¿å­˜ç›®éŒ„å¤±æ•—: {e}")
            self.save_directory = "."
    
    def _init_detector(self):
        """åˆå§‹åŒ–ç‰¹å¾µæª¢æ¸¬å™¨ - å¹³è¡¡åƒæ•¸"""
        try:
            # âœ… åŸºæ–¼æœç´¢çµæœ[1]çš„é©ä¸­SIFTåƒæ•¸
            self.detector = cv2.SIFT_create(
                nfeatures=1000,               # é©ä¸­çš„ç‰¹å¾µé»æ•¸é‡
                contrastThreshold=0.04,       # é©ä¸­çš„å°æ¯”åº¦é–€æª»
                edgeThreshold=10,             # æ¨™æº–é‚Šç·£é–€æª»
                sigma=1.6                     # æ¨™æº–sigmaå€¼
            )
            print("âœ… ä½¿ç”¨å¹³è¡¡SIFTæª¢æ¸¬å™¨")
            return True
        except AttributeError:
            self.detector = cv2.ORB_create(nfeatures=1000)
            print("âœ… ä½¿ç”¨ORBæª¢æ¸¬å™¨")
            return False
    
    def _setup_matcher(self):
        """è¨­å®šFLANNåŒ¹é…å™¨"""
        if self.use_sift:
            FLANN_INDEX_KDTREE = 1
            index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
            search_params = dict(checks=50)
            self.matcher = cv2.FlannBasedMatcher(index_params, search_params)
            print("âœ… SIFT + FLANN KD-Tree åŒ¹é…å™¨")
        else:
            FLANN_INDEX_LSH = 6
            index_params = dict(
                algorithm=FLANN_INDEX_LSH,
                table_number=6,
                key_size=12,
                multi_probe_level=1
            )
            search_params = dict(checks=50)
            self.matcher = cv2.FlannBasedMatcher(index_params, search_params)
            print("âœ… ORB + FLANN LSH åŒ¹é…å™¨")
    
    def _load_balanced_templates(self):
        """è¼‰å…¥å¹³è¡¡ç‰ˆæ¨¡æ¿"""
        print("ğŸ“ è¼‰å…¥å¹³è¡¡ç‰ˆæ¨¡æ¿...")
        
        try:
            for item in os.listdir(self.template_dir):
                item_path = os.path.join(self.template_dir, item)
                
                if os.path.isdir(item_path):
                    for filename in os.listdir(item_path):
                        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                            # éæ¿¾æ‰ROIæ¨¡æ¿
                            if not self.enable_roi_templates and '_roi' in filename.lower():
                                continue
                                
                            file_path = os.path.join(item_path, filename)
                            full_name = f"{item}/{filename}"
                            self._process_single_template(file_path, full_name)
                
                elif item.lower().endswith(('.png', '.jpg', '.jpeg')):
                    # éæ¿¾æ‰ROIæ¨¡æ¿
                    if not self.enable_roi_templates and '_roi' in item.lower():
                        continue
                        
                    file_path = os.path.join(self.template_dir, item)
                    self._process_single_template(file_path, item)
            
            original_count = sum(1 for t in self.templates if not self._is_flipped_template(t['name']))
            flipped_count = sum(1 for t in self.templates if self._is_flipped_template(t['name']))
            
            print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.templates)} å€‹å¹³è¡¡æ¨¡æ¿")
            print(f"   åŸå§‹æ¨¡æ¿: {original_count} å€‹")
            print(f"   ç¿»è½‰æ¨¡æ¿: {flipped_count} å€‹")
            print(f"   âŒ ROIæ¨¡æ¿: {'åœç”¨' if not self.enable_roi_templates else 'å•Ÿç”¨'}")
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¨¡æ¿å¤±æ•—: {e}")
    
    def _process_single_template(self, file_path, template_name):
        """è™•ç†å–®å€‹æ¨¡æ¿ - é©ä¸­çš„å“è³ªè¦æ±‚"""
        template_img = self.safe_imread(file_path, cv2.IMREAD_UNCHANGED)
        if template_img is None:
            return
        
        # è™•ç†é€æ˜åœ–
        if len(template_img.shape) == 3 and template_img.shape[2] == 4:
            alpha = template_img[:, :, 3]
            bgr = template_img[:, :, :3]
            bg = np.full(bgr.shape, 200, dtype=np.uint8)
            template_bgr = np.where(alpha[..., None] == 0, bg, bgr)
        else:
            template_bgr = template_img
        
        # è½‰ç°åº¦
        if len(template_bgr.shape) == 3:
            template_gray = cv2.cvtColor(template_bgr, cv2.COLOR_BGR2GRAY)
        else:
            template_gray = template_bgr
        
        # æå–ç‰¹å¾µ
        kp, des = self.detector.detectAndCompute(template_gray, None)
        
        # âœ… é©ä¸­çš„ç‰¹å¾µé»è¦æ±‚
        if des is not None and len(kp) >= 10:  # å¾15é™åˆ°10
            is_flipped = self._is_flipped_template(template_name)
            
            self.templates.append({
                'name': template_name,
                'original_name': template_name,
                'keypoints': kp,
                'descriptors': des,
                'image': template_gray,
                'size': template_gray.shape,
                'is_flipped': is_flipped
            })
            
            direction = "ç¿»è½‰" if is_flipped else "åŸå§‹"
            print(f"   âœ… {template_name}({direction}): {len(kp)} å€‹ç‰¹å¾µé»")
    
    def _is_flipped_template(self, template_name):
        """åˆ¤æ–·æ˜¯å¦ç‚ºç¿»è½‰æ¨¡æ¿"""
        flipped_indicators = ['_flipped', '_flip', '_ç¿»è½‰', '_left', '_L']
        return any(indicator in template_name.lower() for indicator in flipped_indicators)
    
    def detect_monsters(self):
        """ä¸»è¦æª¢æ¸¬æ–¹æ³• - å¹³è¡¡ç‰ˆ"""
        start_time = time.time()
        # è¼‰å…¥å ´æ™¯åœ–
        scene_img = capture_bluestacks()
        if scene_img is None:
            print("âŒ ç„¡æ³•è¼‰å…¥å ´æ™¯åœ–")
            return []
        
        # èƒŒæ™¯è™•ç†
        if self.enable_background_removal:
            print(f"ğŸ¨ é–‹å§‹å¹³è¡¡èƒŒæ™¯è™•ç† ({self.bg_removal_method})...")
            scene_img = self._gentle_background_enhancement(scene_img)
        
        # å ´æ™¯é è™•ç†
        scene_gray = cv2.cvtColor(scene_img, cv2.COLOR_BGR2GRAY)
        
        print(f"ğŸ” é–‹å§‹å¹³è¡¡æ€ªç‰©æª¢æ¸¬...")
        print(f"å ´æ™¯åœ–å°ºå¯¸: {scene_img.shape}")
        
        # ç‰¹å¾µæª¢æ¸¬
        scene_kp, scene_des = self.detector.detectAndCompute(scene_gray, None)
        
        if scene_des is None:
            print("âŒ å ´æ™¯åœ–ç„¡ç‰¹å¾µé»")
            return []
        
        print(f"å ´æ™¯åœ–ç‰¹å¾µé»: {len(scene_kp)} å€‹")
        
        # âœ… å¹³è¡¡çš„æª¢æ¸¬æµç¨‹
        all_detections = self._detect_with_balanced_filtering(scene_kp, scene_des, scene_gray)
        
        # âœ… å¹³è¡¡çš„ç©ºé–“èšé¡
        final_detections = self._balanced_spatial_clustering(all_detections)
        detection_time = time.time() - start_time
        print(f"â±ï¸ æª¢æ¸¬è€—æ™‚: {detection_time:.2f}ç§’")
        # ä¿å­˜æª¢æ¸¬çµæœ
        if self.save_detection_images:
            prefix = "balanced_detection"
            self.save_detection_image(scene_img, final_detections, prefix)
        
        return final_detections
    
    def _gentle_background_enhancement(self, scene_img):
        """ä¿®æ­£ç‰ˆèƒŒæ™¯å¢å¼·"""
        try:
            if self.bg_removal_method == 'gentle_enhancement':
                # è¼•å¾®çš„é«˜æ–¯æ¨¡ç³Šæ¸›å°‘å™ªè²
                blurred = cv2.GaussianBlur(scene_img, (3, 3), 0)
                
                # ä¿®æ­£çš„CLAHE
                lab = cv2.cvtColor(blurred, cv2.COLOR_BGR2LAB)
                lab_planes = list(cv2.split(lab))
                
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                lab_planes[0] = clahe.apply(lab_planes[0])
                
                enhanced = cv2.merge(lab_planes)
                result = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
                
                print("ğŸ¨ å¹³è¡¡èƒŒæ™¯å¢å¼·å®Œæˆ")
                return result
            else:
                return scene_img
                
        except Exception as e:
            print(f"âŒ èƒŒæ™¯è™•ç†å¤±æ•—: {e}")
            return scene_img
    
    def _detect_with_balanced_filtering(self, scene_kp, scene_des, scene_gray):
        """âœ… å¹³è¡¡éæ¿¾çš„æª¢æ¸¬æµç¨‹"""
        all_detections = []
        
        for template in self.templates:
            detection = self._balanced_template_matching(
                template, scene_kp, scene_des, scene_gray
            )
            
            if detection:
                all_detections.append(detection)
        
        # æŒ‰ä¿¡å¿ƒåº¦æ’åº
        all_detections.sort(key=lambda x: x['confidence'], reverse=True)
        
        print(f"ğŸ” å¹³è¡¡æª¢æ¸¬çµæœ: {len(all_detections)} å€‹")
        return all_detections
    
    def _balanced_template_matching(self, template, scene_kp, scene_des, scene_gray):
        """âœ… åŸºæ–¼æœç´¢çµæœ[1]çš„å¹³è¡¡æ¨¡æ¿åŒ¹é…"""
        template_name = template['name']
        template_kp = template['keypoints']
        template_des = template['descriptors']
        is_flipped = template['is_flipped']
        
        try:
            # FLANNåŒ¹é…
            if self.use_sift:
                matches = self.matcher.knnMatch(template_des, scene_des, k=2)
            else:
                template_des_float = np.float32(template_des)
                scene_des_float = np.float32(scene_des)
                matches = self.matcher.knnMatch(template_des_float, scene_des_float, k=2)
            
            # âœ… åŸºæ–¼æœç´¢çµæœ[1]çš„é©ä¸­Lowe's ratio test
            good_matches = []
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    ratio_threshold = 0.7 if self.use_sift else 0.75  # é©ä¸­
                    if m.distance < ratio_threshold * n.distance:
                        good_matches.append(m)
            
            # âœ… åŸºæ–¼æœç´¢çµæœ[1]çš„é©ä¸­åŒ¹é…è¦æ±‚
            MIN_MATCH_COUNT = 5  # å¾8é™åˆ°5
            if len(good_matches) < MIN_MATCH_COUNT:
                return None
            
            # RANSACé©—è­‰
            src_pts = np.float32([template_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
            dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
            
            # âœ… åŸºæ–¼æœç´¢çµæœ[1]çš„æ”¾å¯¬RANSACåƒæ•¸
            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 3.0, 0.95)
            
            if M is None:
                return None
            
            inliers = np.sum(mask)
            inlier_ratio = inliers / len(good_matches)
            
            # âœ… åŸºæ–¼æœç´¢çµæœ[1]çš„é©ä¸­å…§é»è¦æ±‚
            if inlier_ratio < 0.4 or inliers < 4:  # å¾0.6/6é™åˆ°0.4/4
                return None
            
            # è¨ˆç®—æª¢æ¸¬ä½ç½®
            h, w = template['size']
            pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)
            dst_corners = cv2.perspectiveTransform(pts, M)
            
            # âœ… é©ä¸­çš„æª¢æ¸¬æ¡†é©—è­‰
            if not self._balanced_validate_detection_box(dst_corners, scene_gray.shape):
                return None
            
            center_x = int(np.mean(dst_corners[:, 0, 0]))
            center_y = int(np.mean(dst_corners[:, 0, 1]))
            
            # ä½ç½®æœ‰æ•ˆæ€§æª¢æŸ¥
            if not self._is_valid_position(center_x, center_y, scene_gray.shape):
                return None
            
            # âœ… é©ä¸­çš„ä¿¡å¿ƒåº¦è¨ˆç®—
            avg_distance = np.mean([m.distance for m in good_matches])
            base_confidence = (inlier_ratio * len(good_matches) * 0.08) / (avg_distance * 0.01 + 1)
            
            # ç¿»è½‰ç‰ˆæœ¬é™ä½ä¿¡å¿ƒåº¦
            flip_penalty = 0.95 if is_flipped else 1.0
            confidence = base_confidence * flip_penalty
            
            # âœ… é©ä¸­çš„ä¿¡å¿ƒåº¦é–€æª»
            if confidence < self.confidence_threshold:
                return None
            
            detection = {
                'name': template['original_name'],
                'full_name': template_name,
                'position': (center_x, center_y),
                'confidence': confidence,
                'matches': len(good_matches),
                'inliers': int(inliers),
                'inlier_ratio': inlier_ratio,
                'corners': dst_corners,
                'is_flipped': is_flipped,
                'direction': "ç¿»è½‰" if is_flipped else "åŸå§‹",
                'avg_distance': avg_distance,
                'template_name': template['original_name'],
                'match_count': len(good_matches),
                'timestamp': time.time()
            }
            
            direction = "ç¿»è½‰" if is_flipped else "åŸå§‹"
            print(f"   âœ… {template['original_name']}({direction}): ä¿¡å¿ƒåº¦ {confidence:.3f}")
            return detection
            
        except Exception as e:
            return None
    
    def _balanced_spatial_clustering(self, detections):
        """âœ… å¹³è¡¡çš„ç©ºé–“èšé¡"""
        if not detections:
            return []
        
        # âœ… é©ä¸­çš„å“è³ªéæ¿¾
        quality_filtered = [d for d in detections if 
                           d['confidence'] >= self.confidence_threshold and
                           d['inlier_ratio'] >= 0.35 and
                           d['match_count'] >= 4]
        
        # âœ… é©ä¸­çš„ç©ºé–“èšé¡
        clustered_detections = []
        used_positions = []
        
        for detection in quality_filtered:
            position = detection['position']
            is_near_existing = False
            
            for used_pos in used_positions:
                distance = np.sqrt((position[0] - used_pos[0])**2 + (position[1] - used_pos[1])**2)
                
                if distance < self.spatial_distance_threshold:
                    is_near_existing = True
                    break
            
            if not is_near_existing:
                clustered_detections.append(detection)
                used_positions.append(position)
        
        print(f"ğŸ” å¹³è¡¡éæ¿¾: {len(detections)} â†’ {len(quality_filtered)} â†’ {len(clustered_detections)} å€‹")
        return clustered_detections[:4]  # æœ€å¤š4å€‹çµæœ
    
    def _balanced_validate_detection_box(self, corners, scene_shape):
        """âœ… å¹³è¡¡çš„æª¢æ¸¬æ¡†é©—è­‰"""
        try:
            height, width = scene_shape
            points = corners.reshape(-1, 2)
            
            # âœ… é©ä¸­çš„ç¯„åœæª¢æŸ¥
            margin = 100  # é©ä¸­çš„å®¹å¿åº¦
            for point in points:
                x, y = point
                if x < -margin or x > width + margin or y < -margin or y > height + margin:
                    return False
            
            # âœ… é©ä¸­çš„é¢ç©æª¢æŸ¥
            box_area = cv2.contourArea(points)
            scene_area = height * width
            
            # æª¢æ¸¬æ¡†ä¸èƒ½å¤ªå¤§æˆ–å¤ªå°
            if box_area > scene_area * 0.3 or box_area < 200:  # é©ä¸­
                return False
            
            # âœ… é©ä¸­çš„é•·å¯¬æ¯”æª¢æŸ¥
            rect = cv2.minAreaRect(points)
            (center, (w, h), angle) = rect
            
            if w > 0 and h > 0:
                aspect_ratio = max(w, h) / min(w, h)
                if aspect_ratio > 5.0:  # é©ä¸­çš„é•·å¯¬æ¯”
                    return False
            
            return True
            
        except Exception as e:
            return False
    
    def _is_valid_position(self, x, y, shape):
        """ä½ç½®æœ‰æ•ˆæ€§æª¢æŸ¥"""
        height, width = shape
        return not (x < 40 or x > width - 40 or y < 40 or y > height - 40)
    
    def save_detection_image(self, game_frame, detections, prefix="detection_result"):
        """ä¿å­˜æª¢æ¸¬çµæœåœ–ç‰‡"""
        try:
            timestamp = int(time.time())
            
            # ä¿å­˜åŸå§‹æˆªåœ–
            original_filename = os.path.join(self.save_directory, f"original_{timestamp}.png")
            cv2.imwrite(original_filename, game_frame)
            print(f"ğŸ“¸ åŸå§‹æˆªåœ–å·²ä¿å­˜: {original_filename}")
            
            if not detections:
                print("âš ï¸ æ²’æœ‰æª¢æ¸¬çµæœ")
                return
            
            # ç”Ÿæˆå¸¶æª¢æ¸¬æ¡†çš„åœ–ç‰‡
            result_image = self.create_detection_visualization(game_frame, detections)
            
            if result_image is not None:
                result_filename = os.path.join(self.save_directory, f"{prefix}_{timestamp}.png")
                cv2.imwrite(result_filename, result_image)
                print(f"ğŸ¯ æª¢æ¸¬çµæœå·²ä¿å­˜: {result_filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æª¢æ¸¬åœ–ç‰‡å¤±æ•—: {e}")
    
    def create_detection_visualization(self, game_frame, detections):
        """å‰µå»ºå¹³è¡¡ç‰ˆæª¢æ¸¬å¯è¦–åŒ–"""
        try:
            result_image = game_frame.copy()
            
            for i, detection in enumerate(detections):
                # âœ… æ ¹æ“šä¿¡å¿ƒåº¦é¸æ“‡é¡è‰²ï¼ˆé©ä¸­æ¨™æº–ï¼‰
                confidence = detection['confidence']
                if confidence >= 0.15:
                    color = (0, 255, 0)      # ç¶ è‰²ï¼šé«˜ä¿¡å¿ƒåº¦
                elif confidence >= 0.08:
                    color = (0, 255, 255)    # é»ƒè‰²ï¼šä¸­ä¿¡å¿ƒåº¦
                else:
                    color = (255, 0, 255)    # ç´«è‰²ï¼šä½ä¿¡å¿ƒåº¦
                
                # ç•«é‚Šç•Œæ¡†
                corners = detection['corners']
                cv2.polylines(result_image, [np.int32(corners)], True, color, 2)
                
                # ç•«ä¸­å¿ƒé»
                center = detection['position']
                cv2.circle(result_image, center, 6, color, -1)
                
                # æ¨™ç±¤ä¿¡æ¯
                direction = "ç¿»" if detection['is_flipped'] else "åŸ"
                monster_name = detection['name'].split('/')[-1].replace('.png', '')
                label = f"{i+1}.{monster_name}({direction})"
                confidence_label = f"{confidence:.3f}"
                
                # ä¸»æ¨™ç±¤
                cv2.putText(result_image, label, (center[0]-40, center[1]-20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                
                # ä¿¡å¿ƒåº¦æ¨™ç±¤
                cv2.putText(result_image, confidence_label, (center[0]-15, center[1]+15),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
            
            # æ·»åŠ æª¢æ¸¬çµ±è¨ˆä¿¡æ¯
            stats_text = f"å¹³è¡¡æª¢æ¸¬: {len(detections)} éš» | {time.strftime('%H:%M:%S')}"
            cv2.putText(result_image, stats_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            return result_image
            
        except Exception as e:
            print(f"âŒ å‰µå»ºå¯è¦–åŒ–åœ–ç‰‡å¤±æ•—: {e}")
            return None
    
    def safe_imread(self, image_path, flags=cv2.IMREAD_COLOR):
        """å®‰å…¨è®€å–"""
        try:
            img_array = np.fromfile(image_path, dtype=np.uint8)
            return cv2.imdecode(img_array, flags)
        except Exception:
            return None

# åŸ·è¡Œå¹³è¡¡æª¢æ¸¬
if __name__ == '__main__':
    detector = BalancedDetector()
    
    print(f"âœ… å¹³è¡¡æª¢æ¸¬åƒæ•¸:")
    print(f"   èƒŒæ™¯è™•ç†: {'å•Ÿç”¨' if detector.enable_background_removal else 'åœç”¨'}")
    print(f"   è™•ç†æ–¹æ³•: {detector.bg_removal_method}")
    print(f"   ä¿¡å¿ƒåº¦é–€æª»: {detector.confidence_threshold}")
    print(f"   ç©ºé–“èšé¡è·é›¢: {detector.spatial_distance_threshold}")
    print(f"   ROIæ¨¡æ¿: {'å•Ÿç”¨' if detector.enable_roi_templates else 'åœç”¨'}")
    
    while True:
        print("ğŸ”„ æ“·å–ä¸¦è¾¨è­˜ BlueStacks ç•«é¢...")
        results = detector.detect_monsters()
        
        if results:
            print(f"ğŸ‰ æª¢æ¸¬åˆ° {len(results)} éš»æ€ªç‰©:")
            for i, match in enumerate(results):
                direction = "ç¿»è½‰" if match['is_flipped'] else "åŸå§‹"
                print(f"   [{i+1}] {match['template_name']} ({direction})")
                print(f"       ä½ç½®: {match['position']}")
                print(f"       ä¿¡å¿ƒåº¦: {match['confidence']:.3f}")
                print(f"       åŒ¹é…é»æ•¸: {match['match_count']}")
                print(f"       å…§é»æ¯”ä¾‹: {match['inlier_ratio']:.2f}")
        else:
            print("ğŸ˜´ ç„¡æª¢æ¸¬çµæœ")
        
        print("-" * 60)
        time.sleep(2)
